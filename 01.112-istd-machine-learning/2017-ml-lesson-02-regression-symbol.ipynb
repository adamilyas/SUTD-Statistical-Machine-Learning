{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will explore how symbolic software packages, such as Theano, can help us in optimization and machine learning, by automatically computing the gradients of a given function. We will use an artificial set of 50 data points $(x,y)$ where $x = (x_3, x_2, x_1, x_0) \\in \\mathbb{R}^4$ with $x_0 =1$, and $y \\in \\mathbb{R}$. Our goal is to perform gradient descent and linear regression on this data set. First, we load the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "csv = 'https://www.dropbox.com/s/oqoyy9p849ewzt2/linear.csv?dl=1'\n",
    "data = np.genfromtxt(csv,delimiter=',')\n",
    "X = data[:,1:]\n",
    "Y = data[:,0].reshape(-1,1)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the Theano package, and set some global constants. Note that a tensor is a multi-dimensional matrix. The `theano.tensor` module will be used for symbolic computations involving tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "d = X.shape[1] # dimension of feature vectors\n",
    "n = X.shape[0] # number of training samples\n",
    "learn_rate = 0.5 # learning rate for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create some symbolic variables. These symbolic variables act as algebraic objects in Theano, so that Theano will know how to differentiate functions involving these objects. For example, if we create a symbolic variable called $t$, and ask Theano to differentiate the symbolic function $f=t^2$ with respect to $t$, it will return the symbolic function $2t$. The output of the `pp` function is a little difficult to parse. The function `fill(a,b)` creates a matrix/tensor that is the same shape as `a` and fills its entries with `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'theano.tensor.var.TensorVariable'>\n",
      "<class 'theano.tensor.var.TensorVariable'>\n",
      "<class 'theano.tensor.var.TensorVariable'>\n",
      "((fill((t ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (t ** (TensorConstant{2} - TensorConstant{1})))\n"
     ]
    }
   ],
   "source": [
    "from theano import pp      # pp for printing symbolic objects\n",
    "t = T.scalar(name='t')     # symbolic variable\n",
    "f = t**2                   # symbolic function\n",
    "fgrad = T.grad(f, wrt=t)\n",
    "print(type(t))\n",
    "print(type(f))\n",
    "print(type(fgrad))\n",
    "print(pp(fgrad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn a symbolic function into a compiled function. The first argument `[t]` specifies a list of symbolic variables which will become the inputs of the compiled function. It is difficult to display what the compiled function is doing, because the function is represented as a computational graph, so that it can be parallelized later if necessary. By applying the function `pp` to `g.maker.fgraph.outputs[0]`, we get a simplified glimpse of this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "<class 'theano.compile.function_module.Function'>\n",
      "(TensorConstant{2.0} * t)\n"
     ]
    }
   ],
   "source": [
    "g = theano.function([t], fgrad)\n",
    "print(g(3))\n",
    "print(type(g))\n",
    "print(pp(g.maker.fgraph.outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of symbolic variables: scalars, vectors, matrices, and so on. For our linear regression problem, we will create a matrix `x` and a vector `y`. We do not need to specify their dimensions. There is also a special kind of symbolic variable, called a shared variable. It is a symbolic variable that also stores a value for the variable. Theano figures out what kind of tensor to create for the shared variable when you give it a numpy object such a matrix of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = T.matrix(name='x')                       # feature matrix\n",
    "y = T.matrix(name='y')                       # response vector\n",
    "w = theano.shared(np.zeros((d,1)),name='w')  # model parameters\n",
    "print(w.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write down the empirical risk as a symbolic function. Note that we have to use the functions `T.sum` and `T.dot` instead of `np.sum` and `np.dot` to construct this symbolic function. We let Theano compute the gradient of the risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "risk = T.sum((T.dot(x,w) - y)**2)/2/n      # empirical risk\n",
    "grad_risk = T.grad(risk, wrt=w)            # gradient of the risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct a compiled function that performs one step of the gradient descent. It does not take any inputs, and outputs the value of the symbolic function `risk`. Since `risk` depends on symbolic variables `x, y, w`, we need to specify their values. The values of `x, y` are specified by the `givens` argument. The value of `w` will be obtained from that stored in the shared variable. The compiled function also performs an addition step each time it is called. This step involves updating the value of the shared variable `w` with the value `w-learn_rate*grad_risk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model = theano.function(inputs=[],\n",
    "                              outputs=risk,\n",
    "                              updates=[(w, w-learn_rate*grad_risk)],\n",
    "                              givens={x:X, y:Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform the gradient descent algorithm. The compiled function `train_model` is called until the difference between successive training risks/losses is less than the specified tolerance. We also put a limit on the maximum number of iterations in case the gradient descent takes too long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1  loss: 0.758755942272566  diff: 1.860566066312890\n",
      " 2  loss: 0.235428122249730  diff: 0.523327820022836\n",
      " 3  loss: 0.079395760473308  diff: 0.156032361776422\n",
      " 4  loss: 0.030096896364577  diff: 0.049298864108731\n",
      " 5  loss: 0.013695201975103  diff: 0.016401694389473\n",
      " 6  loss: 0.008006092308871  diff: 0.005689109666232\n",
      " 7  loss: 0.005970446508700  diff: 0.002035645800171\n",
      " 8  loss: 0.005225932117259  diff: 0.000744514391441\n",
      " 9  loss: 0.004949550127904  diff: 0.000276381989355\n",
      "10  loss: 0.004845924776166  diff: 0.000103625351738\n",
      "11  loss: 0.004806813029967  diff: 0.000039111746199\n",
      "12  loss: 0.004791984242872  diff: 0.000014828787095\n",
      "13  loss: 0.004786344307211  diff: 0.000005639935661\n",
      "14  loss: 0.004784194268801  diff: 0.000002150038410\n",
      "15  loss: 0.004783373170821  diff: 0.000000821097979\n",
      "16  loss: 0.004783059133879  diff: 0.000000314036943\n",
      "17  loss: 0.004782938874884  diff: 0.000000120258995\n",
      "18  loss: 0.004782892769416  diff: 0.000000046105468\n",
      "19  loss: 0.004782875074349  diff: 0.000000017695067\n",
      "20  loss: 0.004782868276122  diff: 0.000000006798227\n",
      "21  loss: 0.004782865661743  diff: 0.000000002614379\n",
      "22  loss: 0.004782864655366  diff: 0.000000001006377\n",
      "23  loss: 0.004782864267605  diff: 0.000000000387761\n",
      "24  loss: 0.004782864118061  diff: 0.000000000149545\n",
      "25  loss: 0.004782864060334  diff: 0.000000000057727\n",
      "26  loss: 0.004782864038031  diff: 0.000000000022303\n",
      "27  loss: 0.004782864029406  diff: 0.000000000008625\n",
      "28  loss: 0.004782864026068  diff: 0.000000000003338\n",
      "29  loss: 0.004782864024774  diff: 0.000000000001293\n",
      "30  loss: 0.004782864024273  diff: 0.000000000000501\n"
     ]
    }
   ],
   "source": [
    "max_iter = 50\n",
    "num_iter = 1\n",
    "tol = 10**(-12)\n",
    "diff = tol + 1\n",
    "prev_loss = train_model()\n",
    "while (num_iter < max_iter and diff > tol):\n",
    "    loss = train_model()\n",
    "    diff = prev_loss-loss\n",
    "    print('{0:2d}  loss: {1:.15f}  diff: {2:.15f}'.format(num_iter, loss.item(), diff.item()))\n",
    "    prev_loss = loss\n",
    "    num_iter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
