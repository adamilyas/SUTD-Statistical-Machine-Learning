{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "Consider a random variable $X$ on the set $\\{a, b, c, d\\},$ with the following probabilities \n",
    "\n",
    "- $P(X=a)=p_{a}$\n",
    "- $P(X=b)=p_{b}, \\ldots$\n",
    "\n",
    "## Optimal number of bits to encode the possible values of $X$ \n",
    "\n",
    "### Equal Probability\n",
    "If $p_{a}=p_{b}=p_{c}=p_{d}=\\frac{1}{4}$ then on average we expect to use\n",
    "2 bits to transmit a message containing just the value of X using the following encoding\n",
    "- a: 00\n",
    "- b: 01\n",
    "- c: 10\n",
    "- d: 11\n",
    "\n",
    "### Uneven Probability\n",
    "If $p_{a}=\\frac{1}{2}, p_{b}=\\frac{1}{4}, p_{c}=\\frac{1}{8}=p_{d}$, we use a different encoding scheme with the concept\n",
    "\n",
    "we should use fewer bits to encode the more\n",
    "frequently occurring values, and more bits to encode the less\n",
    "frequently occurring ones, example:\n",
    "\n",
    "- a: 0\n",
    "- b: 10\n",
    "- c: 110\n",
    "- d: 111\n",
    "\n",
    "Note that we cannot use shorter codes for b, c or d because\n",
    "we need to be able to unambiguously parse a concatenation of\n",
    "the strings, eg. 1110110 decodes uniquely into dac.\n",
    "\n",
    "With this encoding scheme, on average we use\n",
    "\n",
    "$\\left(\\frac{1}{2} \\times 1\\right)+\\left(\\frac{1}{4} \\times 2\\right)+\\left(\\frac{1}{8} \\times 3\\right)+\\left(\\frac{1}{8} \\times 3\\right)=1.75$\n",
    "bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "The entropy, $H(X)$ of a discrete random variable is given by\n",
    "$$H(X)=-\\sum_{i} p_{i} \\log p_{i}$$\n",
    "where adopt the convention $0 \\log 0 = 0$\n",
    "\n",
    "### Example\n",
    "If $p_{a}=\\frac{1}{2}, p_{b}=\\frac{1}{4}, p_{c}=\\frac{1}{8}=p_{d}$ then the entropy is as follows:\n",
    "\n",
    "$$H(X)=-\\frac{1}{2} \\log _{2} \\frac{1}{2}-\\frac{1}{4} \\log _{2} \\frac{1}{4}-\\frac{1}{8} \\log_2 \\frac{1}{8}-\\frac{1}{8} \\log_2 \\frac{1}{8}=1.75 \\text{ bits}$$\n",
    "\n",
    "This is the same as the average number of bits we\n",
    "computed earlier with our encoding scheme, since we are using a binary encoding\n",
    "\n",
    "### Example 2: Uniform Distribution\n",
    "Entropy $H(X)$ is maximized when X is a\n",
    "uniform distribution; for n classes, we need $- \\log _{2} \\frac{1}{n}$ bits on average to transmit X , and this is the most bandwidth\n",
    "required amongst all possible distributions of X .\n",
    "\n",
    "$$H(X)=-\\frac{1}{4} \\log _{2} \\frac{1}{4}-\\frac{1}{4} \\log _{2} \\frac{1}{4}-\\frac{1}{4} \\log_2 \\frac{1}{4}-\\frac{1}{4} \\log_2 \\frac{1}{4}= - \\log _{2} \\frac{1}{4}=2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "The cross entropy of two discrete distributions p and q  is given by\n",
    "$$H(p, q)=-\\sum_{i} p_{i} \\log q_{i}$$\n",
    "\n",
    "Conditions:\n",
    "- if $q_i = 0 \\Rightarrow p_i = 0$\n",
    "- else: If $q_{i}=0$ for some $i$ but $p_{i}>0,$ then $H(p, q)=\\infty$\n",
    "\n",
    "We can also write $H(X , Y )$ instead when we have two random\n",
    "variables $X$ and $Y$ with distributions $p$ and $q$ respectively.\n",
    "\n",
    "- $H(p, q) \\geq H(p, p)$ for all $q$ (measure of disimilarity)\n",
    "- equality occurs when $q=p.$  (entropy 0 when same)\n",
    "- Not symmetric: i.e. $H(p, q) \\neq H(q, p)$ in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0 -0.0\n",
      "inf inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def CrossEntropy(yHat, y):\n",
    "    if y == 1:\n",
    "        return -np.log(yHat)\n",
    "    else:\n",
    "        return -np.log(1 - yHat)\n",
    "\n",
    "    \n",
    "print(CrossEntropy(0,0), CrossEntropy(1,1))\n",
    "print(CrossEntropy(0,1), CrossEntropy(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler (KL) divergence (or relative entropy)\n",
    "\n",
    "Definition\n",
    "The KL divergence of two discrete distributions p and q such that\n",
    "$q_{i}=0 \\Longrightarrow p_{i}=0,$ is given by\n",
    "\n",
    "$$ \n",
    "\\begin{aligned} D_{K L}(p | q) &=H(p, q)-H(p, p) \\\\ &=\\sum_{i} p_{i} \\log \\frac{p_{i}}{q_{i}} \\end{aligned}\n",
    "$$\n",
    "\n",
    "If $q_{i}=0$ for some $i$ but $p_{i}>0,$ then $H(p, q)=\\infty$\n",
    "\n",
    "KL divergence measures the number of extra bits required to\n",
    "transmit X with distribution p, **as compared to the optimal\n",
    "code**, when we use the sub-optimal coding scheme associated\n",
    "with distribution q.\n",
    "\n",
    "- As with cross entropy, it is not symmetric.\n",
    "- We can use source coding theorem to infer that KL divergence is always non-negative, but a more there is a more direct proof using Jensenâ€™s inequality.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
