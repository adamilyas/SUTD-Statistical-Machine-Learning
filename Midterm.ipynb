{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super set of ML Topics to study: \n",
    "\n",
    "1) Difference between a training and test set. -\n",
    "\n",
    "2) Difference between a predictor function and a loss function. Predictor and loss function for the different problems we want to solve. \n",
    "\n",
    "| Function | Predictor <img width=60/> | Loss <img width=250/>| Exact <img width=150/>| Gradient Descent <img width=200/>|\n",
    "|---|----|----|----|-----|----|\n",
    "|Linear | $\\theta ^T  x + \\theta_0 $ | $\\frac{1}{2} \\sum_{i=1}^{m}\\left(\\left\\langle\\theta, x^{(i)}\\right\\rangle- y^{(i)}\\right)^{2}$ | $ \\hat{\\theta} = (X^{\\top} X)^{-1} X^{\\top} Y$ | $\\theta = \\theta -\\alpha\\left(X^{T} X \\theta-X^{T} y\\right)$\n",
    "| **Ridge** | $\\theta ^T  x + \\theta_0 $ | $\\frac{1}{n} \\sum_{i=1}^{m} \\frac{1}{2}\\left(y^{(i)}-\\theta^{\\top} x^{(i)}\\right)^{2}+\\frac{\\lambda}{2}\\|\\theta\\|^{2}$ | $\\hat{\\theta}=\\left(n \\lambda I+X^{\\top} X\\right)^{-1} X^{\\top} Y$ | $\\left(1-\\eta_{k} \\lambda\\right) \\theta-\\eta_{k}\\left[\\frac{1}{n}\\left(X^{\\top} X\\right) \\theta-\\frac{1}{n} X^{\\top} Y\\right]$\n",
    "| LASS0 | $\\theta ^T  x + \\theta_0 $ | $\\frac{1}{n} \\sum_{i=1}^{m} \\frac{1}{2}\\left(y^{(i)}-\\theta^{\\top} x^{(i)}\\right)^{2}+\\lambda |\\theta|$  | dunno\n",
    "|SVM| Sign of $\\theta^T x$| $\\frac{1}{n} \\sum_{i=1}^{m} \\max \\{1 - y^{(i)} (\\theta^T x^{(i)}) \\}$ | NOOO|\n",
    "|SVM offset| Sign of $\\theta^T + \\theta_0x$| $\\frac{1}{n} \\sum_{i=1}^{m} \\max \\{1 - y^{(i)} (\\theta^T x^{(i)} + \\theta_0) \\}$ | NOOOO :( | no one teach me\n",
    "|SVM error| Sign of $\\theta^T x + \\theta_0$| $\\frac{1}{n} \\sum_{i=1}^{m} \\max \\{1 - y^{(i)} (\\theta^T x^{(i)} + \\theta_0) \\} + \\frac{\\lambda}{2} ||\\theta|| ^2 $| NOOO | No one teach me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( **Ridge regression**  , SVM, SVM with slack and error, Logistic regression, K nearest neighbors, LASSO)\n",
    "\n",
    "### Learn about ridge regression constraints\n",
    "\n",
    "Log-likelihood of Linear Regression\n",
    "$$\\begin{aligned} \\ell(\\theta) &=\\log \\prod_{i=1}^{m} p_{\\theta}\\left(y^{(i)} | x^{(i)}\\right) \\\\ &=\\sum_{i=1}^{m} \\log p_{\\theta}\\left(y^{(i)} | x^{(i)}\\right) \\\\ &=m \\log \\frac{1}{\\sigma \\sqrt{2 \\pi}}-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{m}\\left(y^{(i)}-\\left\\langle\\theta, x^{(i)}\\right\\rangle\\right)^{2} \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Over fitting and Under fitting, when it occurs and how to overcome it, need a few ways, not just one. Definitions, avoidance and change.\n",
    "\n",
    "### Overfitting: Model fits the training data too well as it captures noise in addition to the underlying structure\n",
    "- Low bias, high variance\n",
    "-  **Training error low**, testing error high\n",
    "How to deal:\n",
    "- Increase data size\n",
    "- Early stopping\n",
    "- Decrease model complexity\n",
    "- Adjust regularization\n",
    "\n",
    "### Underfitting: Model is too simple to capture the underlying structure of the data\n",
    "- High bias, low variance\n",
    "- Training error high, testing error high\n",
    "How to deal:\n",
    "- Adjust model complexity: Add more features\n",
    "- Adjust Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Neural Networks, forward passing and back propagation, the chain rule and how one variable can affect another variable.\n",
    "\n",
    "5) Equation for gradient descent, step function and the loss function. \n",
    "\n",
    "### 6) Activation functions. need to know what they are. \n",
    "| Function | $f(z)$ <img width=150/> | $f^{'}(z)$ <img width=150/>|\n",
    "|---------|-------|----------|\n",
    "|Sigmoid $\\sigma(z) $ |$\\frac{1}{1+e^{-z}}$ | $\\sigma^{'}(z) = \\sigma(z) \\sigma(-z)$|\n",
    "|tanh | $\\frac{e^{Z}-e^{-Z}}{e^{Z}+e^{-Z}}$| $1-\\tanh ^{2}(z)$|\n",
    "|ReLU | $f(z)=\\max (0, z)$| $O$ if $z<0$ else $1$ |\n",
    "|sofplus | $f(z)=\\ln \\left(1+e^{z}\\right)$| $$\\frac{1}{1+e^{-z}}$$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.a) Loss Functions\n",
    "$$z^{(i)} = y^{(i)} \\theta^T x^{(i)}$$\n",
    "- Zero One Loss: $\\operatorname{Loss}_{01}(z)=\\mathbb{I} z \\leq 0 \\mathbb{I} = [1 \\text{ if } z \\leq 0 \\text{ else } 0]$\n",
    "- Hinge Loss: $\\operatorname{Loss}_{\\mathrm{H}}(z)=\\max \\{1-z, 0\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to know what are the properties of the activation functions, and where they should be placed in your neural network. Saturated around certain values should be near the ends, near the front should be fast changing. \n",
    "\n",
    "### 7) Max pooling does not change with back propagation, but convulation does. Need to softplus  know how convolution maps onto the orignal feature map and how the output comes out. Need to know stride and padding. Note that padding is a layer of zeroes around the entire original matrix. \n",
    "\n",
    "### 8) For convolution neural network, need to know that sometimes after convolution, we apply an activation function to the feature map to \"rectify\" them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "9) Know how to prove that certain functions are kernels. \n",
    "- Need to first generate $x^{(1)}$ to $x^{(n)}$. \n",
    "- Then form the GRAM matrix $K$ out $(K_{(1,2)} = k(x_1,x_2)$. \n",
    "- Need to check whether this matrix is **symmetric** ($K = K^T$) and positive semi-definite. \n",
    "- Check that $K$ has eigenvalues $\\geq 0$\n",
    "- Check that $z^T Kz \\geq 0$ where z is a (1 x n) matrix ==> $K_{(i,i)} \\geq 0$ for all i. This is equivalent to saying that $K(x_i,x_i) \\geq 0$ for all i. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM + Dual Primal\n",
    "\n",
    "11) SVM primal dual problem, and understand the dual formula and what each term means. Support vectors slide is needed as well, which one are support and non-support vectors. \n",
    "\n",
    "12) Know what is the SVM with error and SVM with offsets. Need to know why it occurs and what increasing lamba does.\n",
    "\n",
    "10) Need to know the primal dual inequalities, need to know how to write down lagrangians and **KKT** conditiions. Min f(x) is the same as Max -f(x). Look at HW problem for lagrangian. \n",
    "\n",
    "### Karush-Kuhn-Tucker (KKT) Conditions\n",
    "$\\begin{array}{ll}{\\text { 1. }} & {\\nabla_{x} L(x, \\alpha)=0} \\\\ {\\text { 2. }} & {g_{1}(x) \\leq 0, \\ldots, g_{m}(x) \\leq 0} \\\\ {\\text { 3. }} & {\\alpha_{1} \\geq 0, \\ldots, \\alpha_{m} \\geq 0} \\\\ {\\text { 4. }} & {\\alpha_{1} g_{1}(x)=0, \\ldots, \\alpha_{m} g_{m}(x)=0}\\end{array}$\n",
    "\n",
    "\n",
    "### Lagrangian (Same for all 3)\n",
    "$$L(\\theta, \\alpha)=\\frac{1}{2}\\|\\theta\\|^{2}+\\sum_{(x, y)} \\alpha_{x, y}\\left(1-y\\left(\\theta^{\\top} x + \\theta_0\\right)\\right)$$\n",
    "\n",
    "| SVM <img width=250/>| (with offset) <img width=250/>|(with error) <img width=250/>|\n",
    "|---|---|----|\n",
    "|PRIMAL|\n",
    "|$\\min {\\frac{1}{2}\\|\\theta\\|^{2}} $|$\\min {\\frac{1}{2}\\|\\theta\\|^{2}} $|$\\min {\\frac{\\lambda}{2}\\|\\theta\\|^{2}} +\\frac{1}{n} \\sum_{(x, y)} \\xi_{x, y} $|\n",
    "|${\\text { s.t }}  {y^{(i)} \\left(\\theta^{\\top} x^{(i)} \\right) \\geq 1}$ for all i| ${y^{(i)} \\left(\\theta^{\\top} x^{(i)} + \\theta_0\\right) \\geq 1}$ for all i| ${\\text { s.t }}  {y^{(i)} \\left(\\theta^{\\top} x^{(i)} \\right) \\geq 1 - \\xi_{x, y}}$ for all i\n",
    "|||$\\xi_{x, y} \\geq 0$ for all i|\n",
    "|DUAL|\n",
    "|max $\\sum_{(x, y)} \\alpha_{x, y}-\\frac{1}{2} \\sum_{(x, y)} \\sum_{\\left(x^{\\prime}, y^{\\prime}\\right)} \\alpha_{x, y} \\alpha_{x^{\\prime}, y^{\\prime}} y y^{\\prime}\\left(x^{\\top} x^{\\prime}\\right)$|max $\\sum_{(x, y)} \\alpha_{x, y}-\\frac{1}{2} \\sum_{(x, y)} \\sum_{\\left(x^{\\prime}, y^{\\prime}\\right)} \\alpha_{x, y} \\alpha_{x^{\\prime}, y^{\\prime}} y y^{\\prime}\\left(x^{\\top} x^{\\prime}\\right)$|max $\\sum_{(x, y)} \\alpha_{x, y}-\\frac{1}{2} \\sum_{(x, y)} \\sum_{\\left(x^{\\prime}, y^{\\prime}\\right)} \\alpha_{x, y} \\alpha_{x^{\\prime}, y^{\\prime}} y y^{\\prime}\\left(x^{\\top} x^{\\prime}\\right)$|\n",
    "|s.t. $\\alpha_{i} \\geq 0$ for all i|$\\alpha_{i} \\geq 0$ for all i|$0 \\leq \\alpha_{i} \\leq \\frac{1}{\\lambda}$ for all i|\n",
    "||$\\sum_{i} \\alpha_{i} y=0$|$\\sum_{i} \\alpha_{i} y=0$|\n",
    "\n",
    "### Parameters\n",
    "$\\hat{\\theta}=\\sum_{(x, y)} \\alpha_{x, y} y x$\n",
    "\n",
    "$\\hat{\\theta}_{0}=y-\\hat{\\theta}^{\\top} x$\n",
    "\n",
    "\n",
    "### Support Vectors\n",
    "$\\hat{\\alpha}_{x, y}>0 : \\quad y\\left(\\hat{\\theta}^{\\top} x\\right)=1$ Support Vector <br>\n",
    "$\\hat{\\alpha}_{x, y}=0 : \\quad y\\left(\\hat{\\theta}^{\\top} x\\right)>1$ Non-Support Vectors\n",
    "\n",
    "### Prediction\n",
    "$h(x ; \\theta)=\\operatorname{sign}\\left(\\theta^{\\top} x\\right)=\\operatorname{sign}\\left(\\sum_{\\left(x^{\\prime}, y^{\\prime}\\right)} \\alpha_{x^{\\prime}, y^{\\prime}} y^{\\prime}\\left(x^{\\top} x^{\\prime}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Graphical models. D-separation, separation for directed and undirected. Need to know all Markov properties and conditional independances you can get from it. Structural equations and how to convert into graph and vice versa.\n",
    "\n",
    "### Markov Random Fields\n",
    "\n",
    "Factorization: $$ \n",
    "p(x)=\\prod_{C \\in c} \\phi c\\left(x_{C}\\right)\n",
    " $$\n",
    " \n",
    "Global Markoc: For any disjoint A, B, S of V, $$ \n",
    "A \\perp B\\left|S \\quad \\Rightarrow \\quad X_{A} \\perp X_{B}\\right| X_{S}\n",
    " $$\n",
    "\n",
    "Local Markov: For all $v \\in V$ \n",
    "$$X_{v} \\perp X_{V \\backslash c l(v)}\\left|X_{\\mathrm{bd}(v)}\\right.\n",
    " $$\n",
    " \n",
    "Pairwise: For all pairs of non-adjency vertices $(v, v^{'})$ in $G$, \n",
    "\n",
    "$$ \n",
    "X_{v} \\perp X_{v^{\\prime}}\\left|V \\backslash\\left\\{v, v^{\\prime}\\right\\}\\right.\n",
    " $$\n",
    "which means $X_v \\perp$ Rest except itself and neighbours given its neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) Need to know how to do the expected value of the activation question in the HW\n",
    ".\n",
    "15) Need to know how to do the conditional means of the covariance matrix. slide 10 lesson 7. \n",
    "16) Need to understand Gaussian processes with the prediction and the covariance matrix depending on the kernel functions. Need to know the prediction term,\n",
    "\n",
    "**Lesson 7 slide 21 and 22** How in the world ... don't understand dude.\n",
    "\n",
    "\n",
    "\n",
    "17) Need to calculate the new C^n matrix and the mean and variance.\n",
    "$$C^{n+1}=\\left[ \\begin{array}{ll}{C^{n}} & {k} \\\\ {k^{T}} & {c}\\end{array}\\right]$$ where \n",
    "$k=\\left[K\\left(x^{(1)}, x^{(n+1)}\\right), \\ldots, K\\left(x^{(n)}, x^{(n+1)}\\right)\\right]^{T}$\n",
    "and $c=K\\left(x^{(n+1)}, x^{(n+1)}\\right)$\n",
    "\n",
    "\n",
    "Hence \n",
    "$$ \n",
    "p\\left(y^{(n+1)} | y^{(1)}, \\ldots, y^{(n)}\\right) \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)\n",
    " $$\n",
    "where \n",
    "$\\mu=k^{T}\\left(C^{n}\\right)^{-1} \\mathbf{y}_{n}$ and \n",
    "$\\sigma^{2}=c-k^{T}\\left(C^{n}\\right)^{-1} k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
